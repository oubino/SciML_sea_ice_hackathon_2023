{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JcO15PWJ4buE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "JcO15PWJ4buE",
    "outputId": "3922035d-fab5-4a82-df8b-397ff9d71641"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from IPython.display import Image\n",
    "Image(url='https://i.imgur.com/DeB62Nl.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IYasnTqQ4FCZ",
   "metadata": {
    "id": "IYasnTqQ4FCZ"
   },
   "source": [
    "* [Kaggle Competition Page](https://www.kaggle.com/competitions/leeds-sciml-sea-ice-segmentation)\n",
    "* [Weights & Biases SciML Leeds team](https://wandb.ai/sciml-leeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BydyQmZdGwiJ",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "BydyQmZdGwiJ",
    "outputId": "55fdb8a5-de1e-41b6-98ba-45192d9251e8"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/U4amljFGkiw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NbXUeSmjgKoC",
   "metadata": {
    "id": "NbXUeSmjgKoC"
   },
   "source": [
    "# Setup\n",
    "\n",
    "First, let's set up the environment by installing the required libraries and importing the necessary modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yXegUgyNhAjt",
   "metadata": {
    "id": "yXegUgyNhAjt"
   },
   "source": [
    "* Create a Kaggle account: If you don't already have a Kaggle account, go to the Kaggle website (https://www.kaggle.com/) and sign up for an account.\n",
    "\n",
    "* Generate the kaggle.json file: After logging in to Kaggle, go to your account settings page. Scroll down to the section titled \"API\" and click on the \"Create New API Token\" button. This will download a file named kaggle.json to your computer.\n",
    "\n",
    "* Upload kaggle.json to the runtime environment. Use the file upload below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K8izr03zyzZZ",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "K8izr03zyzZZ",
    "outputId": "575c9ec3-b02b-4cea-80be-c24a42efd582"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    if fn != 'kaggle.json': print('Are you sure you\\'ve uploaded the right file?')\n",
    "    print(f'Uploaded file \"{fn}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6WgQ8utJy8NB",
   "metadata": {
    "id": "6WgQ8utJy8NB"
   },
   "outputs": [],
   "source": [
    "#Make a directory named kaggle and copy the kaggle.json file there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "# change the permission of the file\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uhIV871Bw3dS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhIV871Bw3dS",
    "outputId": "44bb132a-5f92-49f8-9cab-2490545ba01b"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d spiruel/leeds-sciml-seaice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jg1dAefHyAL2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jg1dAefHyAL2",
    "outputId": "ef7516a3-50a1-48fb-f85d-b28da6b20917"
   },
   "outputs": [],
   "source": [
    "!unzip leeds-sciml-seaice.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GMNpTVDY35go",
   "metadata": {
    "id": "GMNpTVDY35go"
   },
   "source": [
    "# 🚀 Installing and importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db04d9de-1357-4a3b-a80c-f8e6f938440d",
   "metadata": {
    "id": "db04d9de-1357-4a3b-a80c-f8e6f938440d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms.functional as TF\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ItMfw_jWi",
   "metadata": {
    "id": "b70ItMfw_jWi"
   },
   "source": [
    "The `patch_paths` variables are arrays containing the names of the patch files.\n",
    "Each patch patch has a corresponding SAR image, multispectral image, and label. (Except we've not given you the labels for the test patch paths).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a7f52ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a7f52ed",
    "outputId": "f0b6c228-6be0-4562-ac6b-85651191fa30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Patch20190104_218_1200_840', 'Patch20190205_75_1680_960',\n",
       "       'Patch20191216_308_1560_1313', ..., 'Patch20190101_46_960_240',\n",
       "       'Patch20190104_50_960_1200', 'Patch20191216_212_1080_1560'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = 'sciml'\n",
    "train_patch_paths = np.array([os.path.basename(i[:-9]) for i in glob.glob(f'{DATA_DIR}/train/*_sar.tiff')])\n",
    "test_patch_paths = np.array([os.path.basename(i[:-9]) for i in glob.glob(f'{DATA_DIR}/test/*_sar.tiff')])\n",
    "\n",
    "#display patch paths\n",
    "train_patch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "Ns1toXTi1TPH",
   "metadata": {
    "id": "Ns1toXTi1TPH"
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class IceSegmentationDataset(Dataset):\n",
    "    def __init__(self, patch_paths, data_dir, split='train'):\n",
    "        self.patch_paths = patch_paths\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        patch_path = self.patch_paths[index]\n",
    "        sar_path = os.path.join(self.data_dir, self.split, patch_path + '_sar.tiff')\n",
    "        ms_path = os.path.join(self.data_dir, self.split, patch_path + '_vis.tiff')\n",
    "        if self.split != 'test':\n",
    "            label_path = os.path.join(self.data_dir, self.split, patch_path + '_ref.tiff')\n",
    "            label_arr = self.load_img(label_path)\n",
    "            y = label_arr\n",
    "\n",
    "        sar_arr = self.load_img(sar_path)\n",
    "        ms_arr = self.load_img(ms_path)\n",
    "\n",
    "        X_sar = np.expand_dims(sar_arr, 0)[:, ::3, ::3]\n",
    "        X_ms = ms_arr\n",
    "\n",
    "        if self.split != 'test':\n",
    "            return (X_sar, X_ms), y\n",
    "        else:\n",
    "            return (X_sar, X_ms), None\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = tifffile.imread(path)\n",
    "        if len(img.shape) == 3:\n",
    "          img = img.transpose((2, 0, 1))  # Transpose to match PyTorch tensor shape (C, H, W)\n",
    "        return torch.from_numpy(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "Q9pQxS02aXFV",
   "metadata": {
    "id": "Q9pQxS02aXFV"
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_patch_paths, val_patch_paths = train_test_split(train_patch_paths, test_size=0.2, random_state=42)\n",
    "train_dataset = IceSegmentationDataset(train_patch_paths, DATA_DIR)\n",
    "val_dataset = IceSegmentationDataset(val_patch_paths, DATA_DIR)\n",
    "test_dataset = IceSegmentationDataset(test_patch_paths, DATA_DIR, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "072d1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_int = np.array([])\n",
    "ms_int_r = np.array([])\n",
    "ms_int_g = np.array([])\n",
    "ms_int_b = np.array([])\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    (X_sar, X_ms), y = train_dataset.__getitem__(i)\n",
    "    sar_int = np.append(sar_int, X_sar.ravel())\n",
    "    ms_int_r = np.append(ms_int_r, X_ms[0,:,:].ravel())\n",
    "    ms_int_g = np.append(ms_int_g, X_ms[1,:,:].ravel())\n",
    "    ms_int_b = np.append(ms_int_b, X_ms[2,:,:].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33f700a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3556416964232872 0.5520505638405484\n",
      "-0.2089460941649551 0.6538063835970725\n",
      "-0.2977986399965673 0.6143507026534764\n",
      "-0.801564234287415 0.16891951952490908\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(ms_int_r), np.std(ms_int_r))\n",
    "print(np.mean(ms_int_g), np.std(ms_int_g))\n",
    "print(np.mean(ms_int_b), np.std(ms_int_b))\n",
    "print(np.mean(sar_int), np.std(sar_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae0e777d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sar_hist \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mhist(sar_int)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "sar_hist = plt.hist(sar_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_hist = plt.hist(ms_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n6emsiJwAjsq",
   "metadata": {
    "id": "n6emsiJwAjsq"
   },
   "source": [
    "The U-Net model architecture is a convolutional neural network that consists of an encoder and a decoder.\n",
    "The encoder extracts features from the input images, while the decoder upsamples the features to generate the final segmentation map.\n",
    "\n",
    "The model is compiled with the Adam optimiser and binary cross-entropy loss.\n",
    "During training, the validation data is used to monitor the performance of the model and early stopping is applied to prevent overfitting.\n",
    "\n",
    "The training progress can be logged using the wandb library, which allows tracking and visualisation of metrics in a web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b099c85-bcb8-4d7b-9dd3-7f0559988ee1",
   "metadata": {
    "id": "1b099c85-bcb8-4d7b-9dd3-7f0559988ee1"
   },
   "outputs": [],
   "source": [
    "# Define the U-Net model architecture\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(4, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.bridge = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(64, 1, kernel_size=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_sar, input_ms):\n",
    "        concat = torch.cat((input_sar, input_ms), dim=1)\n",
    "        enc1 = self.encoder(concat)\n",
    "        bridge = self.bridge(enc1)\n",
    "        dec1 = self.decoder(bridge)\n",
    "        output = self.output(dec1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75cd17-64a9-4d0f-969b-213044600b84",
   "metadata": {
    "id": "db75cd17-64a9-4d0f-969b-213044600b84"
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "model = UNet()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import torchvision.transforms as tf\n",
    "from torch.nn.functional import threshold, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da258b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sar, ms), label = train_dataset.__getitem__(0)\n",
    "#sar = torch.tensor(sar)\n",
    "#print(sar.shape, type(sar))\n",
    "resize = tf.Resize((1024,1024))\n",
    "ms = resize(ms)\n",
    "print(ms.shape, type(ms))\n",
    "#print(label.shape, type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cca837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define segment anything model\n",
    "\n",
    "sam_checkpoint = \"models/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimiser & loss\n",
    "\n",
    "optimiser = torch.optim.Adam(sam.mask_decoder.parameters())\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train loop\n",
    "\n",
    "def train(sam_model, img, mask):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_embedding = sam_model.image_encoder(img)\n",
    "        sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=mask,\n",
    "        )\n",
    "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "      image_embeddings=image_embedding,\n",
    "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "      sparse_prompt_embeddings=sparse_embeddings,\n",
    "      dense_prompt_embeddings=dense_embeddings,\n",
    "      multimask_output=False,\n",
    "    )\n",
    "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "    binary_mask = normalize(threshold(upscaled_masks, 0.0, 0)).to(device)\n",
    "    loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train loop\n",
    "\n",
    "def evaluate(sam_model, img, gt_mask):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_embedding = sam_model.image_encoder(img)\n",
    "        sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "      image_embeddings=image_embedding,\n",
    "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "      sparse_prompt_embeddings=sparse_embeddings,\n",
    "      dense_prompt_embeddings=dense_embeddings,\n",
    "      multimask_output=False,\n",
    "    )\n",
    "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "    prediction = normalize(threshold(upscaled_masks, 0.0, 0)).to(device)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d_rn880_3fwQ",
   "metadata": {
    "id": "d_rn880_3fwQ"
   },
   "source": [
    "# ✅ Sign Up\n",
    "\n",
    "Sign up to a free [Weights & Biases account here](https://wandb.ai/signup)\n",
    "\n",
    "[Weights and Biases docs](https://docs.wandb.ai/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83f318-0dae-4745-b31e-b202cf3f3028",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa83f318-0dae-4745-b31e-b202cf3f3028",
    "outputId": "db9688b2-6766-4eaf-829d-a588e91d48a4"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (input_sar, input_ms), labels in tqdm(train_loader):\n",
    "        input_sar = input_sar.to(device)\n",
    "        input_ms = input_ms.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(input_sar, input_ms)\n",
    "        outputs = outputs[:,:,::4,::4].squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * input_sar.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (input_sar, input_ms), labels in tqdm(val_loader):\n",
    "            input_sar = input_sar.to(device)\n",
    "            input_ms = input_ms.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_sar, input_ms)\n",
    "            outputs = outputs[:,:,::4,::4].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * input_sar.size(0)\n",
    "\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize a new W&B run\n",
    "# wandb_enabled = False\n",
    "\n",
    "# earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "# if wandb_enabled:\n",
    "#     wandb.init(entity='sciml-leeds', project='sea-ice-segmentation')\n",
    "#     history = model.fit(train_generator,\n",
    "#           validation_data=val_generator,\n",
    "#           epochs=10,\n",
    "#           use_multiprocessing=True,\n",
    "#           workers=6,\n",
    "#           callbacks=[earlystopper, WandbCallback()])\n",
    "#     wandb.finish()\n",
    "# else:\n",
    "#     history = model.fit(train_generator,\n",
    "#           validation_data=val_generator,\n",
    "#           epochs=10,\n",
    "#           use_multiprocessing=True,\n",
    "#           workers=6,\n",
    "#           callbacks=[earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3mIFvCa8axLj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "3mIFvCa8axLj",
    "outputId": "e3f21de1-4dd7-498e-cda8-93ba3b87eff6"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (input_sar, input_ms), _ in test_loader:\n",
    "        input_sar = input_sar.to(device)\n",
    "        input_ms = input_ms.to(device)\n",
    "\n",
    "        outputs = model(input_sar, input_ms)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "# Convert predictions to binary mask\n",
    "predictions = (predictions > 0.5).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qFKBK4tuEqf4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "qFKBK4tuEqf4",
    "outputId": "cd2d03ef-e03e-4221-f959-c1e81bbeff42"
   },
   "outputs": [],
   "source": [
    "#@title Now plot the results of the training run:\n",
    "\n",
    "metric_to_plot = 'loss' #@param ['loss', 'accuracy', 'mean_io_u']\n",
    "\n",
    "ys = history.history[metric_to_plot]\n",
    "ys_val = history.history['val_'+metric_to_plot]\n",
    "num_epochs = len(ys)\n",
    "plt.plot(range(num_epochs), ys, label='train')\n",
    "plt.plot(range(num_epochs), ys_val, label='val')\n",
    "plt.legend()\n",
    "plt.ylabel(metric_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyQ1RUjkCYk8",
   "metadata": {
    "id": "wyQ1RUjkCYk8"
   },
   "source": [
    "Let's dive into the visualization of our sea ice segmentation results using the validation dataset. By running the code snippet provided, we obtain valuable insights into the model's performance and its ability to accurately identify sea ice boundaries.\n",
    "\n",
    "First, the model makes predictions on the validation dataset using the model.predict() function. The resulting predictions are stored in the variable preds. These predictions represent the segmentation masks, indicating the presence or absence of sea ice in the images.\n",
    "\n",
    "Next, we extract a single batch of data from the validation generator using next(iter(val_generator)). This batch consists of SAR (Synthetic Aperture Radar) images, multispectral (MS) images, and their corresponding ground truth segmentation masks. These components are unpacked into the variables X_sar_test, X_ms_test, and y_test, respectively.\n",
    "\n",
    "Now, we proceed to visualize the results. Within a loop, we iterate over the predictions stored in preds. For each prediction, we create a figure with a grid layout of four subplots.\n",
    "\n",
    "The first subplot, labeled \"SAR Image,\" displays the SAR image from the validation set. SAR imagery provides valuable information about the surface characteristics of sea ice.\n",
    "\n",
    "The second subplot, labeled \"MS Image,\" showcases the multispectral image captured during the validation process. Multispectral data enables us to analyze sea ice from different spectral perspectives.\n",
    "\n",
    "Moving on to the third subplot, titled \"Segmentation Mask,\" we observe the model's predicted segmentation mask. The thresholding operation (`preds[i,:,:,0]>threshold`) helps us visualize the predicted sea ice boundaries clearly.\n",
    "\n",
    "Lastly, the fourth subplot, labeled \"Ground Truth,\" presents the actual ground truth segmentation mask obtained from the validation dataset. This serves as a reference to assess the accuracy of our model's predictions.\n",
    "\n",
    "By carefully examining these subplots, we can compare the model's predictions against the ground truth and gain insights into the performance and limitations of our sea ice segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd545dd",
   "metadata": {
    "id": "1dd545dd"
   },
   "outputs": [],
   "source": [
    "#@markdown You need to specify the threshold value, which determines the threshold for classifying pixels as foreground or background. Adjusting this value may affect the performance of the segmentation.\n",
    "\n",
    "threshold = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb5504-13b0-40f0-87f8-956eee759e23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "05cb5504-13b0-40f0-87f8-956eee759e23",
    "outputId": "bee3187e-def8-4c24-b319-ed1e0d5270db"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(val_generator,steps=1)\n",
    "\n",
    "(X_sar_test, X_ms_test), y_test = next(iter(val_generator))\n",
    "\n",
    "for i in range(preds.shape[0]):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(X_sar_test[i,:,:,:])\n",
    "    plt.title('SAR Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(X_ms_test[i,:,:,:])\n",
    "    plt.title('MS Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(preds[i,:,:,0]>threshold)\n",
    "    plt.title('Segmentation Mask')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(y_test[i,:,:,0])\n",
    "    plt.title('Ground Truth')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CKj__AgcCp7j",
   "metadata": {
    "id": "CKj__AgcCp7j"
   },
   "source": [
    "* Are there any notable differences between the predicted segmentation mask and the ground truth?\n",
    "* How does adjusting the threshold value impact the visualisation of the segmentation mask?\n",
    "* Can you identify any challenging areas where the model struggles to accurately predict sea ice boundaries?\n",
    "* What potential implications can accurate sea ice segmentation have on environmental research and decision-making processes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8tne9Y6E4AYh",
   "metadata": {
    "id": "8tne9Y6E4AYh"
   },
   "source": [
    "# Generate Submission\n",
    "Ta-da! 📝🎉 Our model is trained and ready, and it's time to run our predictions on the test set and create the submission file.\n",
    "\n",
    "A submission file named 'submission.csv' is created to store the predictions.\n",
    "For each image in the test set, the image ID and run-length encoded pixels are written to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ZJjVt8Nik",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e4ZJjVt8Nik",
    "outputId": "98e160dc-485f-4715-ccb5-018aa6ad2a1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#ref:https://www.kaggle.com/paulorzp/run-length-encode-and-decode.\n",
    "def rle_encode(mask):\n",
    "    '''\n",
    "    mask: numpy array binary mask\n",
    "    1 - mask\n",
    "    0 - background\n",
    "    Returns encoded run length\n",
    "    '''\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# Generate submission DataFrame\n",
    "submission_df = pd.DataFrame(columns=['ImageId', 'EncodedPixels'])\n",
    "\n",
    "for i, patch_path in enumerate(test_patch_paths):\n",
    "    image_id = os.path.basename(patch_path)\n",
    "    mask = predictions[i, :, :, 0] > threshold\n",
    "    encoded_pixels = rle_encode(mask)\n",
    "    submission_df.loc[i] = [image_id, encoded_pixels]\n",
    "\n",
    "# Save submission DataFrame to CSV file\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5XpckGbq8SGW",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "1b0640a4a5d4414a9eddf11fc7238d1a",
      "b6335de2b5914928babe04d4f0ee2718",
      "98efb35d17ea4465abb991c9747c2279",
      "8d66d1d33bfd4e2384db2ec7dd174dd3",
      "15d854c700a0441ab0a549e83f3e99c7"
     ]
    },
    "id": "5XpckGbq8SGW",
    "outputId": "c1e77c44-fcb3-4c9e-86c0-3ebdf070d552"
   },
   "outputs": [],
   "source": [
    "#@title Click to download submission file\n",
    "\n",
    "from google.colab import files\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "button = widgets.Button(description=\"Download csv\", width=200, height=200)\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    files.download('submission.csv')\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(button, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GTUt7j6K9RH3",
   "metadata": {
    "id": "GTUt7j6K9RH3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15d854c700a0441ab0a549e83f3e99c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b0640a4a5d4414a9eddf11fc7238d1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Download csv",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_b6335de2b5914928babe04d4f0ee2718",
      "style": "IPY_MODEL_98efb35d17ea4465abb991c9747c2279",
      "tooltip": ""
     }
    },
    "8d66d1d33bfd4e2384db2ec7dd174dd3": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_15d854c700a0441ab0a549e83f3e99c7",
      "msg_id": "",
      "outputs": []
     }
    },
    "98efb35d17ea4465abb991c9747c2279": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "b6335de2b5914928babe04d4f0ee2718": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
